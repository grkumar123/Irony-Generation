{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**https://github.com/tuhinjubcse/SarcasmGeneration-ACL2020**"
      ],
      "metadata": {
        "id": "78Uk9Yl9umU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Git Clone**"
      ],
      "metadata": {
        "id": "tAnk5CO3ue-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0XREYOzsJiZ",
        "outputId": "3ca3bc9a-4244-4bb4-d60d-a0ae4853b55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SarcasmGeneration-ACL2020'...\n",
            "remote: Enumerating objects: 424, done.\u001b[K\n",
            "remote: Counting objects: 100% (424/424), done.\u001b[K\n",
            "remote: Compressing objects: 100% (313/313), done.\u001b[K\n",
            "remote: Total 424 (delta 202), reused 279 (delta 100), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (424/424), 309.86 KiB | 17.21 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tuhinjubcse/SarcasmGeneration-ACL2020.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**To just reverse the valence of the input (REVERSE)**"
      ],
      "metadata": {
        "id": "Nfa8cx-qsL4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U PyYAML\n",
        "import yaml\n",
        "yaml.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y2sRidjJtFOi",
        "outputId": "66edbeda-19c9-43ff-d648-eaa98b3c25d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'6.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4wzAizDstCp",
        "outputId": "fc39033c-7952-45c5-8d9d-1beceea944b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "path = '/content/SarcasmGeneration-ACL2020/comet-commonsense/config/config.yaml'\n",
        "\n",
        "def loadConfigForROV():\n",
        "\twith open(path) as f:\n",
        "\t\tdocs = yaml.load_all(f, Loader=yaml.FullLoader)\n",
        "\t\tfor doc in docs:\n",
        "\t\t\tfor k, v in doc.items():\n",
        "\t\t\t\tif k==\"exception_vadarneg_words\":\n",
        "\t\t\t\t\texception_vadarneg_words=v\n",
        "\t\t\t\telif k==\"missing_vadarneg_words\":\n",
        "\t\t\t\t\tmissing_vadarneg_words=v\n",
        "\treturn exception_vadarneg_words,missing_vadarneg_words\n",
        "\n",
        "def loadConfigForRank():\n",
        "\twith open(path) as f:\n",
        "\t\tdocs = yaml.load_all(f, Loader=yaml.FullLoader)\n",
        "\t\tfor doc in docs:\n",
        "\t\t\tfor k, v in doc.items():\n",
        "\t\t\t\tif k==\"swap\":\n",
        "\t\t\t\t\tswap_words=v\n",
        "\treturn swap_words\n",
        "\n",
        "def loadConfigForSentences():\n",
        "\twith open(path) as f:\n",
        "\t\tdocs = yaml.load_all(f, Loader=yaml.FullLoader)\n",
        "\t\tfor doc in docs:\n",
        "\t\t\tfor k, v in doc.items():\n",
        "\t\t\t\tif k==\"nonoverlap\":\n",
        "\t\t\t\t\tnonoverlap_words=v\n",
        "\treturn nonoverlap_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def loadConfigForRetrieval():\n",
        "\twith open(path) as f:\n",
        "\t\tdocs = yaml.load_all(f, Loader=yaml.FullLoader)\n",
        "\t\tfor doc in docs:\n",
        "\t\t\tfor k, v in doc.items():\n",
        "\t\t\t\tif k==\"stop_words\":\n",
        "\t\t\t\t\tstop_words = v[0].split(',')\n",
        "\t\t\t\tif k==\"missing\":\n",
        "\t\t\t\t\tmissing = v\n",
        "\t\t\t\tif k==\"quantifier\":\n",
        "\t\t\t\t\tquantifier = v\n",
        "\t\t\t\tif k==\"replacement\":\n",
        "\t\t\t\t\treplacements = v\n",
        "\t\t\t\tif k==\"start\":\n",
        "\t\t\t\t\tstart = v\n",
        "\t\t\t\tif k==\"wrongNE\":\n",
        "\t\t\t\t\twrongNE = v\n",
        "\n",
        "\treturn stop_words,missing,quantifier,replacements,start,wrongNE\n",
        "\n",
        "\n",
        "def loadConfig(step):\n",
        "\tif step == 'ROV':\n",
        "\t\treturn loadConfigForROV()\n",
        "\telif step == 'Retrieve':\n",
        "\t\treturn loadConfigForRetrieval()\n",
        "\telif step == 'Sentences':\n",
        "\t\treturn loadConfigForSentences()\n",
        "\telse:\n",
        "\t\treturn loadConfigForRank()"
      ],
      "metadata": {
        "id": "viKiPewXsnwr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "#from loadconfig import loadConfig\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "def getWordNetAntonyms():\n",
        "\tm= {}\n",
        "\tfor line in open('/content/SarcasmGeneration-ACL2020/data/antonyms.txt'):\n",
        "\t\tm[line.strip().split()[0]]= line.strip().split()[1]\n",
        "\treturn m\n",
        "\n",
        "\n",
        "def findIfnegationPresent(utterance):\n",
        "\twords = utterance.split()\n",
        "\tfor w in words:\n",
        "\t\tif w=='not' or w=='never' or  w=='Not' or w=='Never':\n",
        "\t\t\treturn w,True\n",
        "\treturn '',False\n",
        "\n",
        "def findIfendingwithnt(utterance):\n",
        "\n",
        "\td = {\"didn't\": \"did\",\"don't\": \"do\",\"doesn't\":\"does\",\"can't\": \"can\",\n",
        "\t\"cannot\":\"can\",\"wouldn't\":\"would\",\"shouldn't\":\"should\"}\n",
        "\twords = utterance.split()\n",
        "\tfor w in words:\n",
        "\t\tif w in d:\n",
        "\t\t\treturn w,d[w],True\n",
        "\t\tif w.lower() in d:\n",
        "\t\t\treturn w,d[w.lower()].capitalize(),True\n",
        "\treturn '','',False\n",
        "\n",
        "\n",
        "def getAntonym(word):\n",
        "\tantonyms = getWordNetAntonyms()\n",
        "\tif word.lower() not in antonyms:\n",
        "\t\tsynonymsset = []\n",
        "\t\tantonymsset = []\n",
        "\t\tfor syn in wn.synsets(word.lower()):\n",
        "\t\t\tfor l in syn.lemmas():\n",
        "\t\t\t\tsynonymsset.append(l.name())\n",
        "\t\t\t\tif l.antonyms():\n",
        "\t\t\t\t\tantonymsset.append(l.antonyms()[0].name())\n",
        "\t\tif len(antonymsset)==0:\n",
        "\t\t\tfor w in synonymsset:\n",
        "\t\t\t\tif w in antonyms:\n",
        "\t\t\t\t\treturn antonyms[w.lower()]\n",
        "\t\t\treturn \"not \"+word\n",
        "\t\telse:\n",
        "\t\t\treturn antonymsset[0]\n",
        "\telse:\n",
        "\t\treturn antonyms[word.lower()]\n",
        "\n",
        "\n",
        "def ifTwoNegation(utterance):\n",
        "\texception_vadarneg_words, missing_vadarneg_words= loadConfig('ROV')\n",
        "\tutterance = utterance.replace(',','')\n",
        "\tsid = SentimentIntensityAnalyzer()\n",
        "\tarr = []\n",
        "\tsent = word_tokenize(utterance)\n",
        "\tfor i in range(len(sent)):\n",
        "\t\tw = sent[i]\n",
        "\t\tif w == 'no':\n",
        "\t\t\tcontinue\n",
        "\t\tss = sid.polarity_scores(w)\n",
        "\t\tif (ss['neg']==1.0 or w in missing_vadarneg_words) and (w not in exception_vadarneg_words):\n",
        "\t\t\tarr.append((w,i,abs(ss['compound'])))\n",
        "\tif len(arr)==2:\n",
        "\t\tif abs(arr[0][1]-arr[1][1])==2:\n",
        "\t\t\treturn [arr[0][0],arr[1][0]],True\n",
        "\t\telse:\n",
        "\t\t\treturn [arr[1][0]],True\n",
        "\telse:\n",
        "\t\treturn [],False\n",
        "\n",
        "\n",
        "def isThereOnlyOneNegation(utterance):\n",
        "\texception_vadarneg_words, missing_vadarneg_words= loadConfig('ROV')\n",
        "\tsid = SentimentIntensityAnalyzer()\n",
        "\tcount = 0\n",
        "\tword = ''\n",
        "\tarr = []\n",
        "\tfor w in word_tokenize(utterance):\n",
        "\t\tif w=='no':\n",
        "\t\t\tcontinue\n",
        "\t\tss = sid.polarity_scores(w)\n",
        "\t\tif ss['neg']==1.0 and w not in exception_vadarneg_words:\n",
        "\t\t\tcount = count+1\n",
        "\t\t\tif count<=1:\n",
        "\t\t\t\tword = w\n",
        "\t\t\tarr.append(word)\n",
        "\t\telif w in missing_vadarneg_words and count==0:\n",
        "\t\t\tcount = count+1\n",
        "\t\t\tif count<=1:\n",
        "\t\t\t\tword = w\n",
        "\tif count==1:\n",
        "\t\treturn word,True\n",
        "\treturn 'cant_change',False\n",
        "\n",
        "\n",
        "\n",
        "#TODO In future work used improved negation\n",
        "#Current style/sentiment transfer techniques are still at low accuracy\n",
        "def reverse_valence(utterance):\n",
        "\t#directly handle these without going for complicated logic\n",
        "\tutterance = utterance.lower()\n",
        "\tutterance = utterance.replace(' i ',' I ')\n",
        "\tif 'should be' in utterance or 'would be' in utterance:\n",
        "\t\treturn utterance.replace(' be ',' not be ').capitalize()\n",
        "\tif ' need to ' in utterance:\n",
        "\t\treturn utterance.replace(' need to ',' need not ').capitalize()\n",
        "\tif 'hate' in utterance:\n",
        "\t\treturn utterance.replace('hate','love').capitalize()\n",
        "\tif 'least' in utterance:\n",
        "\t\treturn utterance.replace('least','most').capitalize()\n",
        "\tif utterance.endswith('lies.'):\n",
        "\t\treturn utterance.replace('lies','truth').capitalize()\n",
        "\n",
        "\tutterance = utterance.replace(\" don't \",\" do not \")\n",
        "\n",
        "\t#check if negation present , in terms of single or double words or not/n't words\n",
        "\tword,verdict = findIfnegationPresent(utterance)\n",
        "\tnegword,replneg,verdict1 = findIfendingwithnt(utterance)\n",
        "\twords,verdict3 = ifTwoNegation(utterance)\n",
        "\tnegative, verdict2 = isThereOnlyOneNegation(utterance)\n",
        "\n",
        "\t#handle case by case , give priority to remove not first\n",
        "\t# print(\"here1\",utterance)\n",
        "\tif verdict == True:\n",
        "\t\treturn utterance.replace(word+' ','')\n",
        "\telif verdict1==True and verdict2==False:\n",
        "\t\treturn utterance.replace(negword,replneg)\n",
        "\telif verdict3==True:\n",
        "\t\tfor w in words:\n",
        "\t\t\tif getAntonym(w).startswith('not'):\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tutterance = utterance.replace(w,getAntonym(w))\n",
        "\t\treturn utterance\n",
        "\telse:\n",
        "\t\tprev_utterance = utterance\n",
        "\t\tutterance = utterance.replace(negative,getAntonym(negative))\n",
        "\t\t#incase algorithm could not handle still try to negate\n",
        "\t\t#cases replace present tense verbs by appending a don't\n",
        "\t\t#cases replace unique words prefixing with un \n",
        "\t\tif utterance == prev_utterance:\n",
        "\t\t\ttext = word_tokenize(utterance)\n",
        "\t\t\tpos_text = pos_tag(text)\n",
        "\t\t\tfor a,b in pos_text:\n",
        "\t\t\t\tif b == 'VBP':\n",
        "\t\t\t\t\tutterance = utterance.replace(a,\"don't \"+a)\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\tif a.startswith('un'):\n",
        "\t\t\t\t\tutterance = utterance.replace(a,a[2:])\n",
        "\t\t\t\t\tbreak\n",
        "\t\tutterance = utterance.split()\n",
        "\t\tfor i in range(len(utterance)):\n",
        "\t\t\tif utterance[i] == 'an' and utterance[i+1][0] not in ['a','e','i','o','u']:\n",
        "\t\t\t\tutterance[i] = 'a'\n",
        "\t\tutterance = ' '.join(utterance)\n",
        "\t\treturn utterance.capitalize()\n",
        "\n",
        "#print(reverse_valence(sys.argv[1]))"
      ],
      "metadata": {
        "id": "piM5-IxEsNJa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3BfPIGJ3K9n",
        "outputId": "2cf4de09-69af-46e7-b5f7-9a5fb8c13d53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_valence(sys.argv[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAABR0jessFy",
        "outputId": "94b3075d-caf1-4e3a-b10b-e20bb2a5507c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_valence('I love music'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrt_Bn6QuB7U",
        "outputId": "804cafdb-16b6-404a-88ef-3da2ca30736e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't love music\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_valence('I am happy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc7j6YLhuB96",
        "outputId": "156dfdd8-9f67-4eb0-dec2-870b668669d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't am happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_valence('Today it will rain at 3 p.m'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc0ootB1uCAy",
        "outputId": "79101eda-5379-4c74-8f31-e65872a1f80e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today it will rain at 3 p.m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_valence('I am sad'))"
      ],
      "metadata": {
        "id": "MaGm8tCfuxdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60eadc4a-4ee6-4ef1-8a1f-ca52a7085a9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am cheerful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_valence('I will go to market at night'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKko2Z-82x86",
        "outputId": "49124f08-c2d5-4811-8537-b71a9235a8b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I will go to market at night\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_valence('I do not like to eat pizza much.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRluEjKI3XFt",
        "outputId": "5a114e57-72c8-4ea9-d698-a569d3a7550c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i do like to eat pizza much.\n"
          ]
        }
      ]
    }
  ]
}